---
author: khongai
ogImage: /public/assets/2025/10/30/0_neuron_intro.jpg
pubDatetime: 2025-10-30T15:22:00Z
modDatetime: 2025-10-30T16:52:45.934Z
title: AI 101 - Ph·∫ßn 3, M·∫°ng N∆° ron (Neural Network) v√† H·ªçc s√¢u (Deep Learning)
slug: ai-101-phan3-neural-network-and-deep-learning
featured: false
draft: false
tags:
  - AI
description:
  B√†i vi·∫øt n√†y ch√∫ng ta s·∫Ω c√πng nhau t√¨m hi·ªÉu ti·∫øp c√°c kh√°i ni·ªám v·ªÅ M·∫°ng Neron (Neural Network) v√† H·ªçc s√¢u (Deeplearning)
---


![AI ML Flow](/assets/2025/10/25/flow.png)

>üëâ Bu·ªïi h√¥m nay s·∫Ω n√≥i s·∫Ω v·ªÅ Neural Network v√† Deep Learning!

C≈©ng kh√¥ng qu√™n nh·∫Øc l·∫°i h√¨nh v·∫Ω v·ªÅ "ƒë·∫°i gia ƒë√¨nh AI" ƒë·ªÉ m∆∞·ªùng t∆∞·ª£ng v·ªÅ m·ªëi quan h·ªá gi·ªØa c√°i kh√°i ni·ªám

![1 ai](/assets/2025/10/30/1_neural_network.png)

# N√£o ng∆∞·ªùi v√† c∆° ch·∫ø ghi nh·ªõ th√¥ng tin

Tr∆∞·ªõc khi t√¨m hi·ªÉu v·ªÅ m·∫°ng neuron, h√£y b·∫Øt ƒë·∫ßu t·ª´ m·ªôt v√≠ d·ª• th·ª±c t·∫ø:
Khi ta g·∫∑p m·ªôt ng∆∞·ªùi nhi·ªÅu l·∫ßn, n√£o s·∫Ω d·∫ßn ghi nh·ªõ h·ªç th√¥ng qua nhi·ªÅu ƒë·∫∑c ƒëi·ªÉm nh∆∞ khu√¥n m·∫∑t, d√°ng ƒëi hay c√°ch ƒÉn m·∫∑c...

Sau n√†y, khi g·∫∑p ai ƒë√≥ c√≥ m·ªôt v√†i n√©t t∆∞∆°ng ƒë·ªìng, ta th∆∞·ªùng c√≥ c·∫£m gi√°c quen thu·ªôc, nh∆∞ ƒë√£ t·ª´ng g·∫∑p ·ªü ƒë√¢u ƒë√≥.

C√†ng nhi·ªÅu n√©t t∆∞∆°ng ƒë·ªìng, c·∫£m gi√°c th√¢n quen c√†ng hi·ªán h·ªØu ‚Äî nh∆∞ th·ªÉ n√£o ta ƒëang th·ª±c hi·ªán m·ªôt ‚Äúph√©p c·ªông‚Äù c√°c n√©t t∆∞∆°ng ƒë·ªìng ·∫•y.
K·∫øt qu·∫£ c·ªßa ‚Äúph√©p c·ªông‚Äù n√†y c√†ng l·ªõn th√¨ c·∫£m gi√°c quen thu·ªôc c√†ng m·∫°nh ‚Äì gi·ªëng nh∆∞ m·ªôt t√≠n hi·ªáu sinh h·ªçc c√≥ ƒë·ªô ‚Äúm·∫°nh‚Äù hay ‚Äúy·∫øu‚Äù kh√°c nhau.

T√≥m l·∫°i: "C√†ng nhi·ªÅu ƒë·∫∑c ƒëi·ªÉm ƒë∆∞·ª£c ghi nh·ªõ ‚Üí t√≠n hi·ªáu c√†ng m·∫°nh ‚Üí c·∫£m gi√°c quen thu·ªôc c√†ng r√µ"

ƒê·∫øn ƒë√¢y, ta c√≥ th·ªÉ ‚Äúm∆∞·ªùng t∆∞·ª£ng‚Äù v·ªÅ m·ªëi li√™n h·ªá gi·ªØa ‚Äút·ªïng‚Äù c√°c ƒë·∫∑c ƒëi·ªÉm v√† ƒë·ªô m·∫°nh c·ªßa t√≠n hi·ªáu th·∫ßn kinh, t·ª´ ƒë√≥ h√¨nh th√†nh n√™n kh·∫£ nƒÉng ghi nh·ªõ.

Hi·ªán t∆∞·ª£ng n√†y ƒë∆∞·ª£c c√°c nh√† khoa h·ªçc m√¥ t·∫£ th√¥ng qua m√¥ h√¨nh neuron sinh h·ªçc. M·ªói k√Ω ·ª©c hay nh·∫≠n th·ª©c ƒë∆∞·ª£c h√¨nh th√†nh t·ª´ m·ª©c ƒë·ªô li√™n k·∫øt gi·ªØa c√°c t·∫ø b√†o th·∫ßn kinh (neurons) ‚Äì n∆°i c√°c t√≠n hi·ªáu ƒëi·ªán ‚Äì h√≥a h·ªçc ƒë∆∞·ª£c truy·ªÅn qua c√°c kh·ªõp th·∫ßn kinh (synapses).

![2 nao nguoi](/assets/2025/10/30/2_nao_nguoi.png)

Ph√≥ng to li√™n k·∫øt gi·ªØa 2 t·∫ø b√†o th·∫ßn kinh.

![3 lien ket](/assets/2025/10/30/3_link_zoom.png)

Khi con ng∆∞·ªùi h·ªçc t·∫≠p ho·∫∑c tr·∫£i nghi·ªám ƒëi·ªÅu m·ªõi, n√£o s·∫Ω ƒëi·ªÅu ch·ªânh c∆∞·ªùng ƒë·ªô c·ªßa c√°c k·∫øt n·ªëi th·∫ßn kinh, c·ªßng c·ªë nh·ªØng li√™n k·∫øt th∆∞·ªùng xuy√™n ho·∫°t ƒë·ªông v√† l√†m y·∫øu d·∫ßn nh·ªØng li√™n k·∫øt √≠t s·ª≠ d·ª•ng.

Ch√≠nh s·ª± thay ƒë·ªïi li√™n t·ª•c n√†y t·∫°o n√™n c√°c m·∫´u k·∫øt n·ªëi (patterns) trong n√£o, gi√∫p ch√∫ng ta ghi nh·ªõ, nh·∫≠n bi·∫øt v√† ph·∫£n ·ª©ng ng√†y c√†ng ch√≠nh x√°c h∆°n.

# M·∫°ng Neuron (Neural Network) trong h·ªçc m√°y v√† H·ªçc s√¢u (Deep learning)

C∆° ch·∫ø ghi nh·ªõ c·ªßa n√£o ng∆∞·ªùi b√™n tr√™n ƒë∆∞·ª£c m·∫°ng n∆°-ron nh√¢n t·∫°o (Artificial Neural Networks ‚Äì ANN) ‚Äúm√¥ ph·ªèng‚Äù l·∫°i trong m√°y t√≠nh v·ªõi m·ªôt c·∫•u tr√∫c ƒë∆°n gi·∫£n h∆°n, v√† ch√≠nh ƒëi·ªÅu ƒë√≥ l√† ƒëi·ªÉm ‚Äúc·ªët l√µi‚Äù gi√∫p ANN cho ph√©p m√°y t√≠nh th·ª±c hi·ªán nh·ªØng vi·ªác t∆∞∆°ng t·ª± nh∆∞ n√£o con ng∆∞·ªùi.

## C·∫•u tr√∫c c∆° b·∫£n m·ªôt Neural Network

M·ªôt m·∫°ng neural nh√¢n t·∫°o c∆° b·∫£n g·ªìm ba lo·∫°i t·∫ßng/l·ªõp (layers):

![4 neural network](/assets/2025/10/30/04_nerual_network.png)

* L·ªõp ƒë·∫ßu v√†o (Input Layer): Nh·∫≠n d·ªØ li·ªáu ban ƒë·∫ßu.

* ‚ÄúC√°c‚Äù l·ªõp ·∫©n (Hidden Layers): Nh·∫≠n d·ªØ li·ªáu t·ª´ Input Layer. X·ª≠ l√Ω d·ªØ li·ªáu th√¥ng qua h√†ng tri·ªáu ph√©p to√°n ma tr·∫≠n v√† phi tuy·∫øn. Tr·∫£ l·∫°i k·∫øt qu·∫£ t√≠nh to√°n cho Output Layer. Ch√∫ √Ω t·ª´ ‚Äúc√°c‚Äù c√≥ nghƒ©a l√† c√≥ th·ªÉ c√≥ ‚Äúm·ªôt‚Äù ho·∫∑c ‚Äúnhi·ªÅu‚Äù hidden layer.
 
* L·ªõp ƒë·∫ßu ra (Output Layer):  ƒê∆∞a ra k·∫øt qu·∫£ d·ª± ƒëo√°n cu·ªëi c√πng.

* M·ªói Layer g·ªìm nhi·ªÅu v√≤ng tr√≤n, m·ªói v√≤ng tr√≤n (circle) trong s∆° ƒë·ªì g·ªçi l√† m·ªôt node (ho·∫∑c n∆°ron nh√¢n t·∫°o), m√¥ ph·ªèng l·∫°i ho·∫°t ƒë·ªông c·ªßa m·ªôt t·∫ø b√†o th·∫ßn kinh trong n√£o ng∆∞·ªùi.

‚ö†Ô∏è **Deep Learning:**: Neural Network c√≥ s·ªë  Hidden Layers > 4 th√¨ ƒë∆∞·ª£c g·ªçi l√† Deep Learning

## Neural Network ph·∫£n √°nh "t√≠n hi·ªáu" b·∫±ng c√°ch n√†o

ƒê·ªÉ th·ªÉ hi·ªán m·ªëi li√™n h·ªá gi·ªØa c√°c Neuron ng∆∞·ªùi ta s·ª≠ d·ª•ng m·ªôt m√¥ h√¨nh to√°n h·ªçc nh∆∞ sau:

![5 act func](/assets/2025/10/30/05_activation_func.png)


L√∫c n√†y m·ªói n∆°-ron (neuron) trong m·∫°ng ho·∫°t ƒë·ªông gi·ªëng nh∆∞ m·ªôt ƒë∆°n v·ªã t√≠nh to√°n, c·ª• th·ªÉ:

* Nh·∫≠n ƒë·∫ßu v√†o (Input): M·ªói n∆°-ron nh·∫≠n gi√° tr·ªã t·ª´ c√°c n∆°-ron l·ªõp tr∆∞·ªõc ~ x

* T√≠nh to√°n tr·ªçng s·ªë (Weights): M·ªói ƒë·∫ßu v√†o ƒë∆∞·ª£c nh√¢n v·ªõi m·ªôt tr·ªçng s·ªë (weight) nh·∫•t ƒë·ªãnh ~ w

* T·ªïng h·ª£p th√¥ng tin (Summation): C√°c gi√° tr·ªã ƒë·∫ßu v√†o ƒë∆∞·ª£c c·ªông l·∫°i v√† th√™m m·ªôt h·ªá s·ªë ƒëi·ªÅu ch·ªânh (bias) ~ b

* H√†m k√≠ch ho·∫°t (Activation Function): X√°c ƒë·ªãnh xem t√≠n hi·ªáu c√≥ ƒë∆∞·ª£c truy·ªÅn ti·∫øp hay kh√¥ng ~ f. H√†m f n√†y ƒë·∫∑c tr∆∞ng cho t·ª´ng layer, v√† ƒë∆∞·ª£c l·ª±a ch·ªçn t√πy theo m·ª•c ƒë√≠ch c·ªßa m√¥ h√¨nh.

* Truy·ªÅn sang l·ªõp ti·∫øp theo (Output): N·∫øu t√≠n hi·ªáu ƒë·∫ßu ra sau khi qua h√†m k√≠ch ho·∫°t ƒë·ªß m·∫°nh (v∆∞·ª£t ng∆∞·ª°ng), n√≥ s·∫Ω ƒë∆∞·ª£c truy·ªÅn sang l·ªõp k·∫ø ti·∫øp ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω.

## M√¥ h√¨nh X c√≥ h√†ng t·ª∑ tham s·ªë? l√† g√¨

Ta hay nghe n√≥i:

>ChatGPT c√≥ 175 t·ª∑ tham s·ªë,
>
>Claude c√≥ 560 t·ª∑ tham s·ªë‚Ä¶

**Tham s·ªë (parameter)** ch√≠nh l√† t·∫•t c·∫£ **c√°c tr·ªçng s·ªë (weights)** v√† **h·ªá s·ªë ch·ªách (biases)** k·∫øt n·ªëi gi·ªØa neuron v·ªõi neuron.

N·∫øu s·ª≠ d·ª•ng t·ªëi ∆∞u th√¨ "c√†ng nhi·ªÅu tham s·ªë" =>  m·∫°ng c√†ng s√¢u v√† r·ªông => ƒê·ªô ch√≠nh x√°c c√†ng cao

## C∆° ch·∫ø h·ªçc (Learning Mechanism) 

V·ªÅ b·∫£n ch·∫•t, Deep Learning c≈©ng gi·ªëng Machine Learning: m√¥ h√¨nh h·ªçc t·ª´ d·ªØ li·ªáu ƒë·ªÉ t√¨m ra m·ªëi quan h·ªá gi·ªØa ƒë·∫ßu v√†o (X) v√† ƒë·∫ßu ra (Y). ƒêi·ªÉm kh√°c bi·ªát n·∫±m ·ªü c√°ch m·∫°ng neuron t·ª± ƒëi·ªÅu ch·ªânh h√†ng tri·ªáu tr·ªçng s·ªë (weights) v√† h·ªá s·ªë ch·ªách (biases) ƒë·ªÉ d·∫ßn c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c. Qu√° tr√¨nh h·ªçc di·ªÖn ra qua ba b∆∞·ªõc ch√≠nh:

![5 improve](/assets/2025/10/30/06_improve.png)


* Lan truy·ªÅn thu·∫≠n (Forward Propagation): D·ªØ li·ªáu ƒëi qua c√°c l·ªõp m·∫°ng, m·ªói l·ªõp bi·∫øn ƒë·ªïi ƒë·∫ßu v√†o m·ªôt ch√∫t ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng, cho ƒë·∫øn khi ra k·∫øt qu·∫£ d·ª± ƒëo√°n ·ªü l·ªõp cu·ªëi.

* T√≠nh sai s·ªë (Loss Function): So s√°nh k·∫øt qu·∫£ d·ª± ƒëo√°n v·ªõi gi√° tr·ªã th·∫≠t ƒë·ªÉ bi·∫øt m√¥ h√¨nh ‚Äúsai l·ªách bao nhi√™u‚Äù.

* Lan truy·ªÅn ng∆∞·ª£c (Backpropagation): M·∫°ng t√≠nh to√°n ƒë·∫°o h√†m (gradient) v√† t·ª± ƒëi·ªÅu ch·ªânh c√°c tr·ªçng s·ªë theo h∆∞·ªõng gi·∫£m sai s·ªë.

Ba b∆∞·ªõc n√†y ƒë∆∞·ª£c l·∫∑p l·∫°i h√†ng tri·ªáu l·∫ßn (training loop). M·ªói l·∫ßn, m√¥ h√¨nh h·ªçc th√™m m·ªôt ch√∫t t·ª´ sai s·ªë c·ªßa ch√≠nh n√≥ ‚Äî nh·ªù ƒë√≥ c√°c tr·ªçng s·ªë d·∫ßn ƒë∆∞·ª£c t·ªëi ∆∞u, v√† m·∫°ng ng√†y c√†ng ‚Äúth√¥ng minh‚Äù h∆°n.

üí° Ch√≠nh c∆° ch·∫ø l·∫∑p ‚Äì ph·∫£n h·ªìi ‚Äì ƒëi·ªÅu ch·ªânh n√†y gi√∫p m·∫°ng neuron t·ª± c·∫£i thi·ªán m√¥ h√¨nh theo th·ªùi gian, t∆∞∆°ng t·ª± c√°ch n√£o ng∆∞·ªùi c·ªßng c·ªë k·∫øt n·ªëi th·∫ßn kinh khi h·ªçc

## M·ªôt s·ªë thu·∫≠t ng·ªØ li√™n quan Epoch / Batch / Iteration

| Thu·∫≠t ng·ªØ | Nghƒ©a | Li√™n h·ªá v√≠ d·ª• |
| --- | --- | --- |
| **Epoch** | M·ªôt v√≤ng h·ªçc qua to√†n b·ªô d·ªØ li·ªáu | M·ªôt l·∫ßn v·∫Ω l·∫°i to√†n b·ªô ƒë∆∞·ªùng cong |
| **Batch** | M·ªôt nh√≥m nh·ªè d·ªØ li·ªáu trong m·ªói epoch | V·∫Ω t·ª´ng ph·∫ßn nh·ªè c·ªßa ƒë∆∞·ªùng cong |
| **Iteration** | M·ªôt l·∫ßn c·∫≠p nh·∫≠t tr·ªçng s·ªë (sau m·ªói batch) | Ch·ªânh n√©t v·∫Ω sau m·ªói ƒëo·∫°n |

## Self-Supervised Learning

> Self-Supervised Learning (SSL) l√† ph∆∞∆°ng ph√°p h·ªçc m√† m√¥ h√¨nh t·ª± t·∫°o ra ‚Äúnh√£n‚Äù cho ch√≠nh d·ªØ li·ªáu c·ªßa m√¨nh ‚Äî
> kh√¥ng c·∫ßn con ng∆∞·ªùi g√°n nh√£n th·ªß c√¥ng.

üí° M√¥ h√¨nh **t·ª± ƒë·∫∑t b√†i to√°n ‚Äì t·ª± t·∫°o ƒë√°p √°n ‚Äì r·ªìi t·ª± h·ªçc**.

ƒê√¢y l√† l√Ω do g·ªçi l√† **‚Äút·ª± gi√°m s√°t‚Äù** (self-supervised).

## C√°c lo·∫°i m√¥ h√¨nh Deeplearning

| **Lo·∫°i m√¥ h√¨nh** | **·ª®ng d·ª•ng ch√≠nh** |
| --- | --- |
| **1. Convolutional Neural Network (CNN)** | Nh·∫≠n d·∫°ng h√¨nh ·∫£nh, ph√°t hi·ªán v·∫≠t th·ªÉ, ph√¢n ƒëo·∫°n ·∫£nh, th·ªã gi√°c m√°y t√≠nh (Computer Vision) |
| **2. Recurrent Neural Network (RNN)** | D·ª± b√°o chu·ªói th·ªùi gian, x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP), nh·∫≠n d·∫°ng gi·ªçng n√≥i |
| **3. Long Short-Term Memory (LSTM)** | Ph√¢n t√≠ch chu·ªói d·ªØ li·ªáu d√†i, d·ªãch m√°y, d·ª± ƒëo√°n vƒÉn b·∫£n, nh·∫≠n d·∫°ng l·ªùi n√≥i |
| **4. Gated Recurrent Unit (GRU)** | Gi·ªëng LSTM nh∆∞ng nh·∫π h∆°n, d√πng cho d·ª± b√°o th·ªùi gian th·ª±c ho·∫∑c d·ªØ li·ªáu tu·∫ßn t·ª± |
| **5. Autoencoder (AE)** | N√©n d·ªØ li·ªáu, gi·∫£m chi·ªÅu, ph√°t hi·ªán b·∫•t th∆∞·ªùng (anomaly detection), kh·ª≠ nhi·ªÖu ·∫£nh ho·∫∑c t√≠n hi·ªáu |
| **6. Variational Autoencoder (VAE)** | Sinh d·ªØ li·ªáu m·ªõi (·∫£nh, √¢m thanh), h·ªçc bi·ªÉu di·ªÖn ti·ªÅm ·∫©n (latent representation) |
| **7. Generative Adversarial Network (GAN)** | T·∫°o ·∫£nh, video, √¢m thanh, deepfake, tƒÉng d·ªØ li·ªáu hu·∫•n luy·ªán |
| **8. Transformer** | X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, d·ªãch m√°y, t√≥m t·∫Øt, chatbot, sinh vƒÉn b·∫£n (LLM) |
| **9. Vision Transformer (ViT)** | Nh·∫≠n d·∫°ng v√† ph√¢n lo·∫°i ·∫£nh, ph√°t hi·ªán v·∫≠t th·ªÉ, hi·ªÉu ng·ªØ c·∫£nh h√¨nh ·∫£nh |
| **10. Diffusion Model** | Sinh ·∫£nh, video, gi·ªçng n√≥i; v√≠ d·ª•: Stable Diffusion, DALL¬∑E, Midjourney |
| **11. Graph Neural Network (GNN)** | Ph√¢n t√≠ch m·∫°ng x√£ h·ªôi, khuy·∫øn ngh·ªã (recommendation), ph√°t hi·ªán gian l·∫≠n, h√≥a h·ªçc t√≠nh to√°n |
| **12. Mamba Model (SSM-based)** | X·ª≠ l√Ω d·ªØ li·ªáu tu·∫ßn t·ª±, m√¥ h√¨nh ng√¥n ng·ªØ (LLM) hi·ªáu nƒÉng cao, thay th·∫ø Transformer ti·ªÅm nƒÉng |